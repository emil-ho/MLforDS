{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'>\n",
    "<tr>\n",
    "<td style='background-color:white'>\n",
    "    <p align=\"left\">\n",
    "    Exercises for the course<br>\n",
    "        <b>Machine Learning for Data Science</b><br>\n",
    "    Winter Semester 2022/23\n",
    "    </p>\n",
    "</td>\n",
    "<td style='background-color:white'>\n",
    "    G. Montavon<br>\n",
    "    Institute of Computer Science<br>\n",
    "    <b>Department of Mathematics and Computer Science</b><br>\n",
    "    Freie Universit√§t Berlin\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <h1>Exercise Sheet 3 (programming part)</h1>\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy,sklearn,sklearn.datasets,utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will experiment with two different techniques to compute the PCA components of a dataset: Singular Value Decomposition (SVD) and Power Iteration. We consider a random subset of the Labeled Faces in the Wild (LFW) dataset, readily accessible from sklearn, and we apply some basic preprocessing to discount strong variations of luminosity and contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D = sklearn.datasets.fetch_lfw_people(resize=0.5)['images']\n",
    "D = D[numpy.random.mtrand.RandomState(1).permutation(len(D))[:2000]]*1.0\n",
    "D = D - D.mean(axis=(1,2),keepdims=True)\n",
    "D = D / D.std(axis=(1,2),keepdims=True)\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two functions are provided for your convenience and are available in `utils.py` that is included in the zip archive. The functions are the following:\n",
    "\n",
    "* **`utils.scatterplot`** produces a scatter plot from a two-dimensional data set.\n",
    "\n",
    "* **`utils.render`** takes an array of data points or objects of similar shape, and renders them in the IPython notebook.\n",
    "\n",
    "Some demo code that makes use of these functions is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.scatterplot(D[:,32,20],D[:,32,21]) # Plot relation between adjacent pixels\n",
    "utils.render(D[:30],15,2,vmax=5)         # Display first 10 examples in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4  (15 P)\n",
    "\n",
    "Principal components can be found computing a singular value decomposition. Specifically, we assume a matrix $X$ whose columns contain the data points represented as vectors, and where the data points have been centered (i.e. we have substracted to each of them the mean of the dataset). The matrix $X$ is of size $d \\times N$ where $d$ is the number of input features and $N$ is the number of data points. This matrix, more specifically, the rescaled matrix $Z = \\frac{1}{\\sqrt{N}} X$ is then decomposed using singular value decomposition:\n",
    "$$\n",
    "\\textstyle U \\Lambda V = Z\n",
    "$$\n",
    "The $k$ principal components can then be found in the first $k$ columns of the matrix $U$.\n",
    "\n",
    "**(a)** Compute the principal components of the data using the function `numpy.linalg.svd`.\n",
    "\n",
    "**(b)** Measure the computational time required to find the principal components. Use the function `time.time()` for that purpose. Do *not* include in your estimate the computation overhead caused by loading the data, plotting and rendering.\n",
    "\n",
    "**(c)** Plot the projection of the dataset on the first two principal components using the function `utils.scatterplot` and visualize the 60 leading principal components using the function `utils.render`.\n",
    "\n",
    "Note that if the algorithm runs for more than 3 minutes, there may be some error in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### REPLACE BY YOUR CODE\n",
    "import solutions; solutions.pca_svd(D)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the scatter plot, we observe that much more variance is expressed in the first two principal components than in individual dimensions as it was plotted before. When looking at the principal components themselves which we render as images, we can see that the first principal components correspond to low-frequency filters that select for coarse features, and the following principal components capture progressively higher-frequency information and are also becoming more noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (15 P)\n",
    "\n",
    "The first PCA algorithm based on singular value decomposition is quite expensive to compute. Instead, the power iteration algorithm looks only for the first component and finds it using an iterative procedure. It starts with an initial weight vector $\\boldsymbol{w} \\in \\mathbb{R}^d$, and repeatedly applies the update rule\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w} \\leftarrow S \\boldsymbol{w} \\,\\big/\\, \\|S \\boldsymbol{w}\\|.\n",
    "$$\n",
    "\n",
    "where $S$ is the covariance matrix defined as $S = \\frac1N XX^\\top$. Like for standard PCA, the objective that iterative PCA optimizes is $J(\\boldsymbol{w}) = \\boldsymbol{w}^\\top S \\boldsymbol{w}$ subject to the unit norm constraint for $\\boldsymbol{w}$. We can therefore keep track of the progress of the algorithm after each iteration.\n",
    "\n",
    "**(a)** Implement the power iteration algorithm. Use as a stopping criterion the value of $J(\\boldsymbol{w})$ between two iterations increasing by less than 0.01. Print the value of the objective function $J(\\boldsymbol{w})$ at each iteration.\n",
    "\n",
    "**(b)** Measure the time taken to find the principal component.\n",
    "\n",
    "**(c)** Visualize the the eigenvector $\\boldsymbol{w}$ obtained after convergence using the function `utils.render`.\n",
    "\n",
    "Note that if the algorithm runs for more than 1 minute, there may be some error in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### REPLACE BY YOUR CODE\n",
    "import solutions; solutions.pca_powit(D)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the computation time has decreased significantly. The difference of performance becomes larger as the number of dimensions and data points increases. We can observe that the principal component is the same (sometimes up to a sign flip) as the one obtained by the SVD algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
